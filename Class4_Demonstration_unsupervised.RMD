---
title: "Demonstration of Unsupervised Methods"
author: "JAS"
date: 
output:
  github_document: default
  html_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---

# Demonstration of Unsupervised Methods 

We will be using two different datasets to demonstrate different unsupervised machine learning methods. 

* Breast Cancer Imaging data 
    + Data Citation: This breast cancer database was obtained from the University of Wisconsin Hospitals, Madison from Dr.William H. Wolberg. 

* Simulated data that we will use to represent clinical phenotypic data on COPD extracted from an EHR system. 
    + Data Citation: Ultsch, A.: Clustering with SOM: U*C, In Proc. Workshop on Self-Organizing Maps, Paris, France, (2005) , pp. 75-82
    

***

### Load Packages Needed for Both Demonstrations

Ensure that all packages are installed. Note that the package `ggbiplot` is not available in R. We are downloading it from the developer's github repository.You do not need to replicate the use of `ggbiplot`.

```{r load_packages}
#First need to load package devtools and Rtools if you want to load packages from github
# library(devtools)

# install_github("vqv/ggbiplot")
library(ggbiplot)

library(stats) #PCA
library(factoextra) # extract and visualize the output of exploratory multivariate data analyses, such as PCA
library(cluster) 
library(tidyverse)
```


## Demonstration of Principal Components Analysis

First, we will utilize breast cancer imaging data. In this demonstration, rather than trying to predict malignancy, we are interested in determining if we can uncover the underlying constructs of the image that are explained by the nine features. In other words, can we **reduce the number of features** from nine down to some smaller number but still capture the same information. Is there shared information across those features? To accomplish this, we will apply ***principal components analysis (PCA)*** to the feature data within the breast cancer dataset.

***

### Step 1: Load and Prepare Dataset
Remember to replace the file path with the location where the breast cancer data are stored.

```{r prepdata}
bc.data<-read.csv("./Data/breast_cancer_wisconsin.data.txt", header = FALSE)

# There are no informative variable names, so we need to add variable names manually:
var.names<-c("id", "clump_thickness", "uniformity_csize", "uniformity_cshape", "marg_adhesion", "single_ecell_size", "bare_nuclei", "b_chromatin", "normal_nucleoli", "mitoses", "outcome")
colnames(bc.data)<-var.names
str(bc.data)

# Clean data of '?' as in previous demonstration
bc.data[bc.data=="?"]<-NA
bc.data$bare_nuclei<-as.numeric(bc.data$bare_nuclei)

# Restrict to malignant cases because we are only interested in looking similarities in the cases. Is there some underlying information 
# that can help us understand the breast imaging of cases:
bc.data<-bc.data[(which(bc.data$outcome==4)),]

# Strip off the outcome and id variable becasue we only want a dataset with features in it.
bc.data.features<-(bc.data[,2:10])

```

### Step 2: Determine if scaling is necessary

Generally, we want all our data to be on the same scale.
```{r scale}
# Obtain and compare means and standard deviations across features. na.rm removes the missings
colMeans(bc.data.features, na.rm = TRUE)
apply(bc.data.features, 2, sd, na.rm = TRUE)

# If we see some difference in means and standard deviations, we need to center and scale. 
```

### Step 3: Conduct the Principal Components Analysis

The function `prcomp()` will center and scale the variables and then identify the principal components
```{r pca}

bc.pca<-prcomp( ~., data = bc.data.features, center = TRUE, scale = TRUE, na.action = na.omit) # the "~." means include all features in the dataset

# Can compare sds used to scale with the sds above to ensure they are close.
bc.pca$scale

# Generates scree plot
fviz_eig(bc.pca)

# view results of pca. Note the first three components are needed to explain at least 75% of the variance
summary(bc.pca)
# The result shows that there isn't much shared information across features because only PC1 explains 32% of the variance, PC2 and PC3 only explain 10-15%.

# Identify how features loaded on the different components
bc.pca$rotation
# From the results, we can see that "bare_nuclei" is loading positively on PC2 and PC3.

ggbiplot(bc.pca)

ggbiplot(bc.pca, choices = c(2, 3))
# We can see that "bare_nuclei" is increasing in the dimension of PC2 and PC3, which suggests that it is loading strongly on those 2 principal components.
```

Based on the results, maybe PCA is not as good for analysis because there is not much shared variance across the 9 features.

***
## Demonstration of Clustering Analysis

In this demonstration, we will attempt to uncover phenotypic subtypes within clinical data of Chronic Obstructive Pulmonary Disease (COPD). COPD is defined as airflow limitation that is not fully reversible. This is a very broad definition, and it suspected that there are a number of distinct phenotypes within the broader term of COPD. Identifying these subtypes can allow researchers to conduct more targeted investigations of COPD, uncovering mechanisms and risk factors for the different subtypes. This demonstration is loosely based on the work performed by Cho et al. Respiratory Research 2010; 11:30. The data are not the same. Please note that for practical reasons, we are using a small dataset with only 3 variables and 212 patient records. But, this same procedure could be repeated with a larger number of variables and/or records.

For this demonstration, the three variables in our dataset are:
1. post-bronchodilator FEV1 percent predicted
2. percent bronchodilator responsiveness
3. airway wall thickness

Goal: With these 3 features, can we cluster our data to what might represent clinical phenotypes?
***

### Step 1: Load data and prepare for analysis
```{r dataprep2}
copd.data<-read.delim("./Data/Hepta.lrn", header = FALSE)

copd.data<-copd.data[,2:4] # we only want column 2:4 because column 1 is just row number (meaningless)

# Assign column names
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names

# Omit missing data
copd.data.nomiss<-na.omit(copd.data)

# Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)

# The means are small and very close to zero. Standard deviations are very similar. So scaling might not be necessary.
```


### Step 2: Conduct a clustering analysis using k-means clustering

We can use the `kmeans` function in order to identify clusters within the data, based on the three variables. 
With *kmeans clustering*, we must first set the value of *k* (# of clusters).
```{r}
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart = 25)
str(clusters)
clusters
fviz_cluster(clusters, data = copd.data.nomiss)
#Show the mean value of features within each cluster
clusters$centers
```

Remember, we set the # of clusters to be 5, so the algorithm forced all data into 5 clusters.
From the plot, we can see that the yellow cluster is somewhat big and seems like it is made up of 3 different clusters.
So, we should conduct a ***gap_statistic analysis*** to determine optimal number of clusters.
```{r}
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN = kmeans, nstart = 25, K.max = 9, B = 50) # FUN = function; B = # of bootstraps
print(gap_stat, method = "firstmax")
# The result provides us the optimal number of clusters, which is 7 in this demonstration.
# We can look at the result table. At 7 clusters, the value of gap is the highest (0.9583801)
```

Now we can go back to use our `kmeans` function and set seven clusters:
```{r}
clusters.7<-kmeans(copd.data.nomiss, 7, nstart = 25)

str(clusters.7)
clusters.7

fviz_cluster(clusters.7, data = copd.data.nomiss)

# Now the clusters look more reasonable.
```

### Step 3: Conduct a hierarchical clustering analysis

Note there are different methods you can use to create your dissimilarity matrix. We are using *complete linkage* in this demonstration, which tends to produce more compact clusters.
In this demonstration, we are using the top-down method `hclust`, which will grow from the top to the bottom. That means everyone will start from their own cluster and then sort of group together.
```{r}
# Create Dissimilarity matrix
diss.matrix <- dist(copd.data.nomiss, method = "euclidean")

# Hierarchical clustering using Complete Linkage
clusters.h<- hclust(diss.matrix, method = "complete" )

# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)
```

Now that we have the dendrogram, but how do we determine where in the height do we draw to obtain the optimal number of clusters?
Use the ***gap_statistic analysis*** to find out!
```{r}
#create function to use within clusGap
hclusCut <- function(x, k) list(cluster = cutree(hclust(dist(x, method="euclidian"), method="average"), k=k))

gap_stat <- clusGap(copd.data.nomiss, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

# If I don't want to create a function, I can just state FUN = hcut.
gap_stat <- clusGap(copd.data.nomiss, FUN = hcut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

After we obtain the optimal number of clusters, we can use number of clusters from gap statistic to obtain cluster assignment for each observation.
```{r}
clusters.h.7<-cutree(clusters.h, k=7)
table(clusters.h.7)
```

Alternatives for hierarchical clustering:
```{r}
clusters.hcut<-hcut(copd.data.nomiss, k = 5, hc_func = "hclust", hc_method = "single", hc_metric = "euclidian")

clusters.hcut$size
fviz_dend(clusters.hcut, rect=TRUE)
fviz_cluster(clusters.hcut)

gap_stat <- clusGap(copd.data.nomiss, FUN = hcut, hc_method="single", K.max = 10, B = 5)
fviz_gap_stat(gap_stat)

input.feature.vals<-cbind(copd.data.nomiss,cluster=clusters.hcut$cluster)

input.feature.vals %>%
  group_by(cluster) %>%
  summarise_all(mean)

#GENERAL SYNTAX
#input.feature.vals<-cbind(orig.data,cluster=cluster.object$cluster)

#input.feature.vals %>%
 # group_by(`cluster.object$cluster`) %>%
  #summarise_all(mean)

```

