---
title: "Demonstration of Caret for Machine Learning"
author: "JAS"
date: " "
output: 
  github_document: default
  html_document: default
  word_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
## Overview of the Caret Package

The caret package (Classification And REgression Training) contains a number of functions to streamline the process for creating analytic pipelines for prediction. It calls to other libraries to run algorithms, but provides a seamless and uniform interface for working with different algorithms. We will use it often in this class, so I"m going to give a quick overview now.

Primary functionalities of caret include:

* pre-processing
* data splitting
* feature selection
* model tuning using resampling
* variable importance estimation

***

Helpful resources using caret:

Max Kuhn's explainer of the [caret package](https://topepo.github.io/caret/model-training-and-tuning.html)

Kuhn M. Building predictive models in R using the caret package. Journal of Statistical Software 2008;28(5) doi: 10.18637/jss.v028.i05

Webinar, given by Max Kuhn, available on [YouTube](https://www.youtube.com/watch?v=7Jbb2ItbTC4) (~1 hour)

***

### Some useful functions for pre-processing
```{r dataclean}
library(tidyverse)
library(caret)
library(stats)

set.seed(111)

bc_data = read_delim("./breast_cancer_wisconsin.data.txt", col_names = FALSE)

var_names = 
  c("id", "clump_thickness", "uniformity_csize", "uniformity_cshape", "marg_adhesion", "single_ecell_size", "bare_nuclei", "b_chromatin", "normal_nucleoli", "mitoses", "outcome")

colnames(bc_data) = var_names
str(bc_data)

bc_data[bc_data == "?"] = NA
bc_data$bare_nuclei = as_numeric(bc_data$bare_nuclei)
bc_data$id = as.character(bc_data$id)

bc_data$outcome = as.factor(bc_data$outcome)
levels(bc_data$outcome) = c("Benign", "Malignant")
str(bc_data)

summary(bc_data)

missmap(bc_data, main = "Missing values vs observed")

# Remove missings
bc_data = na.omit(bc_data)

# Remove duplicate IDs
bc_data = bc_data %>% distinct(id, .keep_all = TRUE)
```

Computer don't know which data are correlated. To find correlation, we must use numeric data. Then, we calculate correlation using the `cor` function.
Once we have all the correlations, we can feed them into the `findCorrelation` function using any cutoff score that we want. 

Applying *centering* and *scaling* to the dataset is very important for any algorithm that is using distance to create classification.

A *balanced partitions* means that we have the same proportion of outcome in both the training and testing datasets.
```{r preprocess}
#Finding correlated predictors
bc_data_numeric = bc_data %>% dplyr::select(where(is.numeric))

correlations = cor(bc_data_numeric, use = "complete.obs")
high.correlations = findCorrelation(correlations, cutoff = 0.9)
# This tells R to show me all the correlated features at 0.9 or above, and it will store them as an object.

#Remove highly correlated features
new.data.low.corr = bc_data_numeric[,-high.correlations]

#Centering and Scaling
set.up.preprocess = preProcess(bc_data_numeric, method = c("center", "scale"))
#Output pre-processed values
transformed.vals = predict(set.up.preprocess, bc_data_numeric)

set.seed(111)
#Strip off ID var
bc.data$id = NULL

#Creating balanced partitions in the data
train.index = createDataPartition(bc_data$outcome, p = 0.7, list = FALSE)

bc.train = bc_data[train.index,]
bc.test = bc_data[-train.index,]


#Construct k-folds in your data
train.folds = createFolds(bc_data$outcome, k = 10, list = FALSE)

```

### Model Training and Tuning

```{r models}
# See what algorithms Caret can do
names(getModelInfo())

# We can look up some models and see what they can do (i.e. what parameters can be tuned, what can be used (regression, classification, etc.))
modelLookup("rpart")
modelLookup("adaboost")

#Train Function: used for tuning of hyperparameters and choosing "optimal" model

#Use trainControl Function to set method, default is bootstrapping

#Perform 3-fold cross-validation
control.settings<-trainControl(method="cv", number=3)

#Perform repeated 10-fold cross-validation
control.settings.b<-trainControl(method="repeatedcv", number=10, repeats=10)

#Add into train function
set.seed(111)
logit <- train(
 outcome ~ ., data = bc.train, method = "glm", family="binomial",
  trControl = control.settings)

logit
logit$results
logit$resample
logit$finalModel

```

### Model Evaluation

```{r}
#Obtain confusion matrix
test.outcome<-predict(logit, bc.test)
confusionMatrix(test.outcome, bc.test$outcome, positive="Malignant")

#Obtain predicted probabilities
test.outcome.probs<-predict(logit, bc.test, type="prob")

testProbs.rmodel <- data.frame(obs = bc.test$outcome,
                        pred.logit=test.outcome.probs[,2])

#Create calibration plot
calPlotData.rmodel<-calibration(obs ~ pred.logit, data = testProbs.rmodel, class="Malignant", cuts=5)
xyplot(calPlotData.rmodel, auto.key = list(columns = 2))

plot(test.outcome.probs[,2])

#What do I do if my outcome is continuous???

postResample(pred=..., obs=....)
```

