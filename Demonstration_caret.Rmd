---
title: "Demonstration of Caret for Machine Learning"
author: "Lectured by JAS"
output: github_document
---

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Overview of the Caret Package

The caret package (Classification And REgression Training) contains a number of functions to streamline the process for creating analytic pipelines for prediction. It calls to other libraries to run algorithms, but provides a seamless and uniform interface for working with different algorithms.

Primary functionalities of caret include:

* pre-processing
* data splitting
* feature selection
* model tuning using resampling
* variable importance estimation

***

Helpful resources using caret:

Max Kuhn's explainer of the caret package
https://topepo.github.io/caret/model-training-and-tuning.html

Kuhn M. Building predictive models in R using the caret package. Journal of Statistical Software 2008;28(5) doi: 10.18637/jss.v028.i05

Webinar, given by Max Kuhn, available on YouTube (~1 hour): https://www.youtube.com/watch?v=7Jbb2ItbTC4


***
Data Source: UCI Machine Learning Repository, HCV data Dataset

The dataset contains laboratory values of blood donors (control) and Hepatitis C patients with varying levels of liver damage. Created by Lichtinghagen, Klawonn and Hoffmann. Lichtinghagen R et al. J Hepatol 2013; 59: 236-42


Attribute Information:

All attributes except Category and Sex are numerical. The laboratory data are the attributes 5-14.
1) X (Patient ID/No.)
2) Category (diagnosis) (values: '0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis', '2=Fibrosis', '3=Cirrhosis')
3) Age (in years)
4) Sex (f,m)
5) ALB
6) ALP
7) ALT
8) AST
9) BIL
10) CHE
11) CHOL
12) CREA
13) GGT
14) PROT

### Some useful functions for pre-processing outside of the train function

```{r preprocess}
library(tidyverse)
library(caret)
library(stats) #PCA
library(glmnet) # Regularization 

# Read in data on liver function study
set.seed(111)
hcvdat0 <- read.csv("./Data/hcvdat0.csv")


# Make outcome category a factor var
hcvdat0$Category<-as.factor(hcvdat0$Category)

# Collapse factor levels of outcome variable to binary outcome: D+ or D-
hcvdat0$outcome.class<-fct_collapse(hcvdat0$Category, NED=c("0=Blood Donor","0s=suspect Blood Donor"), LiverDisease=c("1=Hepatitis", "2=Fibrosis", "3=Cirrhosis"))

# Drop category and ID variable and remove missing data
hcvdat0$Category<-NULL
hcvdat0$X<-NULL
hcvdat0<-na.omit(hcvdat0)

# Finding correlated predictors to remove variables that are highly correlated
hcvdat.numeric<- hcvdat0 %>% dplyr::select(where(is.numeric)) # Correlation can only be done with numeric variables
correlations<-cor(hcvdat.numeric, use = "complete.obs") # Calculate correlations using `cor` function
high.correlations<-findCorrelation(correlations, cutoff = 0.4) # Find any features that are correlated at 0.4 and above, and store them into this vector called high.correlations

# Remove highly correlated features
new.data.low.corr<-hcvdat.numeric[,-high.correlations]


# Centering and Scaling
set.up.preprocess = preProcess(hcvdat.numeric, method = c("center", "scale"))
# Output pre-processed values to the original dataset
transformed.vals = predict(set.up.preprocess, hcvdat.numeric)

# Creating balanced partitions in the data
# Balanced means same proportion of outcome in both training & testing datasets.
# If we have continuous outcome, then it will split outcome into quantiles and it will ensure it is balanced across quantiles.
train.index = createDataPartition(hcvdat0$outcome.class, p = 0.7, list = FALSE)

hcvdat.train = hcvdat0[train.index,]
hcvdat.test = hcvdat0[-train.index,]


# Construct k-folds in your data (Cross-validation)
train.folds = createFolds(hcvdat0$outcome.class, k = 10, list = FALSE)
# We don't often do this because we usually do cross-validation in the model.
```

### Model Training and Tuning

Using the train function to implement your analytic pipeline.
Train function can be used to implement different algorithms using `method = ` option.
```{r models}
# See what caret can do!
names(getModelInfo())

modelLookup("rpart")
modelLookup("adaboost")

# Train Function: used for tuning of hyperparameters and choosing "optimal" model

# Use trainControl Function to set validation method and options (default is bootstrap)

# Option 1: 10-fold cross-validation
control.settings = trainControl(method = "cv", number = 10)

# Option 2: Repeated 10-fold cross-validation
control.settings.b = trainControl(method = "repeatedcv", number = 10, repeats = 10)

# Option3: Perform sampling to balance data (used when data is not balanced. Eg: 95% of the data has the outcome, only 5% are controls -> unbalanced data)
control.settings.c = trainControl(method = "repeatedcv", number = 10, repeats = 10, sampling = "down")
# `down` sampling = If we have fewer cases, we will take the number of cases and down sample the controls so that we have equal numbers of cases and controls.
# `up` sampling = If we have fewer cases, we will draw from the cases (with replacement), so that we have as many cases as there are controls.
# Many more sampling methods...
```

## Demonstration of LASSO Algorithm using `glmnet`

```{r}
# modelLookup will specify hyperparameters
modelLookup("glmnet") 

set.seed(123) # to ensure reproducibility

lasso <- train(outcome.class ~ ., data = hcvdat.train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings.c)

lasso$results

# Don't depend on defaults for hyperparameters. Add tuning grid for lambda and alpha (but set alpha to 1 for LASSO)
lambda = 10^seq(-3, 1, length = 100)
lambda.grid = expand.grid(alpha = 1, lambda = lambda)

# Incorporate `tuneGrid` into train function 
set.seed(123)

lasso.2 = train(outcome.class ~ ., data = hcvdat.train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings.c, tuneGrid = lambda.grid)

# Use plot to visualize tuning
plot(lasso.2)

lasso.2$bestTune # this will give us the lambda value with the best accuracy. 
```

What if we don't want to look at accuracy, but we want to look at something else, such as sensitivity, specificity, etc.?
```{r}
# `summaryFunction` will allow calculation of sensitivity and specificity, `classProbs= TRUE` will allow the calculation of predicted probabilities
control.settings.d = trainControl(method = "repeatedcv", number = 10, repeats = 5, sampling = "down", classProbs = TRUE, summaryFunction = twoClassSummary)

#Incorporate `tuneGrid` into train function and change evaluation metric to area under ROC curve (default metric is accuracy)
set.seed(123)

lasso.3 = train(
 outcome.class ~ ., data = hcvdat.train, method = "glmnet",preProc = c("center", "scale"), trControl = control.settings.d, tuneGrid = lambda.grid, metric = "ROC")
  
lasso.3$bestTune
```

What if we are concerned about overfitting (due to high accuracy)?
Maybe we want to be close to the best thing we have in our data, but we want to avoid overfitting. 
The `tolerance` function can be used to find a **less complex model** that is sort of close to optimal but trades a little loss of performance for hopefully being more robust to the original data based on (x-xbest)/xbestx 100, which is #the percent difference. 

For example, to select parameter values based on a 2% loss of performance:
```{r}
whichTwoPct = tolerance(lasso.3$results, metric = "ROC", tol = 2, maximize = TRUE) 

lasso.3$results[whichTwoPct, 1:6]
```

### Model Evaluation

```{r}
# Test my model on the test set:
test.outcome = predict(lasso.3, hcvdat.test)

confusionMatrix(test.outcome, hcvdat.test$outcome, positive = "LiverDisease")
```




